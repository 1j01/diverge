{"version":3,"sources":["providers/DatabaseProvider.js","providers/MarkovProvider.js","providers/OriginalJokeProvider.js","phonics.js","evaluators/Assonance.js","app.js","corpus.txt"],"names":["psuedo_db","DatabaseProvider","sumWeights","weights","sum","_iteratorNormalCompletion","_didIteratorError","_iteratorError","undefined","_step","_iterator","values","Symbol","iterator","next","done","value","err","return","weightedSample","sumTotalWeight","n","Math","random","threshold","_iteratorNormalCompletion2","_didIteratorError2","_iteratorError2","_step2","_iterator2","entries","_ref3","_ref2","Object","slicedToArray","key","Markov","order","classCallCheck","this","ngrams","Map","createClass","corpusText","length","slice","index","ngram","set","get","ngram_keys","Array","from","keys","sum_total_weight","current_text","length_to_add","_this","target_length","_loop","i","lastChars","nextCharsToWeights","filter","forEach","MarkovProvider","_ref4","markov","train","_this2","fill","map","continueText","MarkovSelfTextProvider","_ref5","OriginalJokeProvider","suffix","toPhonemes","text","words","toLowerCase","split","word","phonemes","concat","phonemesString","wordsToPhonemes","inVowels","isVowel","match","push","guessPhonemesForWord","findOrGuessPhonemesForWord","Assonance","_ref","maxDistance","minDistanceBeforeCloserIsBetter","closerIsBetterFactor","windowSize","vowelHistory","pointsHistory","points","totalVowels","phoneme","isVowelish","stripStressor","shift","matchIndex","findIndex","recentVowel","testVowelAssonance","matchedRecentVowel","max","phoneme1","phoneme2","normalize","replace","input","document","getElementById","canvas","ctx","getContext","paths","autocompletion_index","cycleAutocompletePaths","direction","path","_visible","getSelectedAutocompletePath","providers","query_providers","path_strings","provider","provider_name","name","providerResults","query","selectionStart","console","error","result","old_paths_left","path_strings_left","j","path_string","k","old_path","string","splice","set_string","Path","judgements","evaluate","sort","path_a","path_b","assonance","view_center_x","font_size","line_height","cursor_blink_timer","cursor_blink_on","previous_text","previous_selection_end","glyph_canvas_map","get_glyph_canvas","char","has","glyph_canvas","createElement","glyph_ctx","font","width","measureText","glyph_width","height","textAlign","textBaseline","fillText","glyphs","x","y","rot","alpha","x_vel","y_vel","rot_vel","alpha_to","prototype","new_string","old_glyphs","old_string","old_string_index","old_char","prev_glyph","simulate","matched","place_y","selection_end_pos","place_x","glyph","x_to","y_to","min","localStorage","e","addEventListener","window","select","execCommand","preventDefault","focus","animate","t","requestAnimationFrame","innerWidth","innerHeight","resize","clearRect","start_pos","end_pos","selectionEnd","lower_pos","upper_pos","before","inside","after","before_width","inside_width","save","translate","fillStyle","fillRect","input_focused","activeElement","hasFocus","indexOf","autoCompleteHilight","globalAlpha","restore","drawImage"],"mappings":"mJACMA,EAAY,CACjB,eACA,8CACA,6CACA,2CACA,4CACA,wCACA,kDACA,6DACA,6EACA,2EACA,2CACA,kBACA,yDACA,qEACA,2FACA,8EACA,oFACA,sFACA,sFACA,0FACA,8BACA,iBACA,wCACA,qBACA,2CACA,0CACA,2CACA,gDACA,6BACA,4BACA,4BACA,6BACA,6BACA,0BACA,uCACA,gCACA,oBACA,sBACA,0BACA,qBACA,4CACA,yDACA,2DACA,2BACA,wBACA,yBACA,+BACA,kCACA,gEACA,oBACA,gCACA,gHACA,0HACA,mEACA,sEACA,wBACA,iEACA,YACA,6BACA,qBACA,ylBACA,+GAGoBC,kGAEnB,OAAOD,kBCpEHE,EAAa,SAACC,GACnB,IAAIC,EAAM,EADoBC,GAAA,EAAAC,GAAA,EAAAC,OAAAC,EAAA,IAE9B,QAAAC,EAAAC,EAAoBP,EAAQQ,SAA5BC,OAAAC,cAAAR,GAAAI,EAAAC,EAAAI,QAAAC,MAAAV,GAAA,EAAsC,CACrCD,GADqCK,EAAAO,OAFR,MAAAC,GAAAX,GAAA,EAAAC,EAAAU,EAAA,YAAAZ,GAAA,MAAAK,EAAAQ,QAAAR,EAAAQ,SAAA,WAAAZ,EAAA,MAAAC,GAK9B,OAAOH,GAGFe,EAAiB,SAAChB,EAASiB,GAChC,IAAMC,EAAIC,KAAKC,SAAWH,EACtBI,EAAY,EAFkCC,GAAA,EAAAC,GAAA,EAAAC,OAAAnB,EAAA,IAGlD,QAAAoB,EAAAC,EAAiC1B,EAAQ2B,UAAzClB,OAAAC,cAAAY,GAAAG,EAAAC,EAAAf,QAAAC,MAAAU,GAAA,EAAoD,KAAAM,EAAAH,EAAAZ,MAAAgB,EAAAC,OAAAC,EAAA,EAAAD,CAAAF,EAAA,GAAxCI,EAAwCH,EAAA,GAEnD,GAAIX,GADJG,GADmDQ,EAAA,IAGlD,OAAOG,GANyC,MAAAlB,GAAAS,GAAA,EAAAC,EAAAV,EAAA,YAAAQ,GAAA,MAAAI,EAAAX,QAAAW,EAAAX,SAAA,WAAAQ,EAAA,MAAAC,KAWtCS,EAAb,WACC,SAAAA,EAAYC,GAAQJ,OAAAK,EAAA,EAAAL,CAAAM,KAAAH,GACnBG,KAAKF,MAAQA,EACbE,KAAKC,OAAS,IAAIC,IAHpB,OAAAR,OAAAS,EAAA,EAAAT,CAAAG,EAAA,EAAAD,IAAA,QAAAnB,MAAA,SAMO2B,GACL,GAAIA,EAAWC,OAAS,EAEvB,IADAD,GAAcA,EAAWE,MAAM,EAAGN,KAAKF,OAChCM,EAAWC,OAASL,KAAKF,OAC/BM,GAAcA,EAAWE,MAAM,EAAGN,KAAKF,OAIzC,IADA,IAAMO,EAASD,EAAWC,OACjBE,EAAQ,EAAGA,EAAQF,EAASL,KAAKF,MAAOS,IAAS,CACzD,IAAMC,EAAQJ,EAAWE,MAAMC,EAAOA,EAAQP,KAAKF,OACnDE,KAAKC,OAAOQ,IAAID,GAAQR,KAAKC,OAAOS,IAAIF,IAAU,GAAK,GAExDR,KAAKW,WAAaC,MAAMC,KAAKb,KAAKC,OAAOa,QACzCd,KAAKe,iBAAmBpD,EAAWqC,KAAKC,UAnB1C,CAAAL,IAAA,eAAAnB,MAAA,SAsBcuC,EAAcC,GAAe,IAAAC,EAAAlB,KACnCmB,EAAgBH,EAAaX,OAASY,EACxCD,EAAaX,OAASL,KAAKF,QAG9BkB,GAAgBpC,EAAeoB,KAAKC,OAAQD,KAAKe,mBAElD,IAPyC,IAAAK,EAAA,SAOhCC,GACR,IAAMC,EAAYN,EAAaV,MAAMU,EAAaX,OAASa,EAAKpB,MAAQ,GAClEyB,EAAqB,IAAIrB,IAC/BgB,EAAKP,WACHa,OAAO,SAAChB,GACR,OAAOc,IAAcd,EAAMF,MAAM,EAAGY,EAAKpB,MAAQ,KASjD2B,QAAQ,SAACjB,GACTe,EAAmBd,IAAID,EAAMF,MAAMY,EAAKpB,MAAQ,GAAIoB,EAAKjB,OAAOS,IAAIF,MAEtEQ,GACCpC,EAAe2C,EAAoB5D,EAAW4D,KAC9C3C,EAAesC,EAAKjB,OAAQiB,EAAKH,mBAnB1BM,EAAI,EAAGA,EAAIJ,GAAiBD,EAAaX,OAASc,EAAeE,IAAKD,IAsB/E,OAAOJ,MAnDTnB,EAAA,GAuDqB6B,aACpB,SAAAA,EAAAC,GAAiC,IAApB7B,EAAoB6B,EAApB7B,MAAOM,EAAauB,EAAbvB,WAAaV,OAAAK,EAAA,EAAAL,CAAAM,KAAA0B,GAChC1B,KAAK4B,OAAS,IAAI/B,EAAOC,GACzBE,KAAK4B,OAAOC,MAAMzB,qDAEbY,EAAcT,GAAO,IAAAuB,EAAA9B,KAG1B,OAAO,IAAIY,MAAM,GAAGmB,KAAK,GAAGC,IAAI,kBAAKF,EAAKF,OAAOK,aAAajB,EAAc,eAIjEkB,EAAb,WACC,SAAAA,EAAAC,GAAqB,IAARrC,EAAQqC,EAARrC,MAAQJ,OAAAK,EAAA,EAAAL,CAAAM,KAAAkC,GACpBlC,KAAKF,MAAQA,EAFf,OAAAJ,OAAAS,EAAA,EAAAT,CAAAwC,EAAA,EAAAtC,IAAA,QAAAnB,MAAA,SAIOuC,EAAcT,GACnB,IAAMqB,EAAS,IAAI/B,EAAOG,KAAKF,OAI/B,OAHA8B,EAAOC,MAAMb,GAGN,IAAIJ,MAAM,GAAGmB,KAAK,GAAGC,IAAI,kBAAKJ,EAAOK,aAAajB,EAAc,UATzEkB,EAAA,GCtFqBE,aACpB,SAAAA,IAAe1C,OAAAK,EAAA,EAAAL,CAAAM,KAAAoC,GAGdpC,KAAKqC,OAAS,sEAETrB,EAAcT,GACnB,MAAO,CAACS,EAAe,IAAMhB,KAAKqC,+BC2C7B,SAASC,EAAWC,GAC1B,IAAMC,EAAQD,EAAKE,cAAcC,MAAM,6BACrClB,OAAO,SAACmB,GAAD,OAASA,IAEdC,EAAW,GAIf,OAHAJ,EAAMf,QAAQ,SAACkB,GACdC,EAAWA,EAASC,OAlBf,SAAoCF,GAC1CA,EAAOA,EAAKF,cAGZ,IAAMK,EAAiBC,EAAgBJ,GACvC,OAAIG,EACIA,EAAeJ,MAAM,KA1CvB,SAA8BC,GACpCA,EAAOA,EAAKF,cAMZ,IAFA,IAAMG,EAAW,GACbI,GAAW,EACN3B,EAAI,EAAGA,EAAIsB,EAAKtC,OAAQgB,IAAK,CACrC,IAAI4B,GAAU,IAEbA,IADGN,EAAKtB,GAAG6B,MAAM,YAEI,MAAZP,EAAKtB,OACV2B,IAGOL,EAAKtB,EAAI,KAAMsB,EAAKtB,EAAI,GAAG6B,MAAM,cAU5BF,GAAkB,IAAN3B,GAC5BuB,EAASO,KAAK,KAEVF,IAAYD,GAAkB,IAAN3B,GAC5BuB,EAASO,KAAK,KAEfH,EAAWC,EAEZ,OAAOL,EAYAQ,CAAqBT,GASAU,CAA2BV,MAEhDC,MCtDaU,aACpB,SAAAA,EAAAC,GAAkF,IAArEC,EAAqED,EAArEC,YAAaC,EAAwDF,EAAxDE,gCAAiCC,EAAuBH,EAAvBG,qBAAuBhE,OAAAK,EAAA,EAAAL,CAAAM,KAAAsD,GAEjFtD,KAAK2D,WAAaH,EAClBxD,KAAKyD,gCAAkCA,EACvCzD,KAAK0D,qBAAuBA,uDAkBpBnB,GAAM,IAAArB,EAAAlB,KACR4C,EAAWN,EAAWC,GACtBqB,EAAe,GACfC,EAAgB,GAClBC,EAAS,EACTC,EAAc,EAuBlB,OAtBAnB,EAASnB,QAAQ,SAACuC,GACjB,GAAIC,qBAAWC,wBAAcF,KAAyB,MAAZA,EAAiB,CACtDJ,EAAavD,OAASa,EAAKyC,aAC9BC,EAAaO,QACbN,EAAcM,SAEf,IAAMC,EAAaR,EAAaS,UAAU,SAACC,GAC1C,OAAOhB,EAAUiB,mBAAmBD,EAAaN,KAE5CQ,EAAqBJ,GAAc,GAAiB,MAAZJ,EAC9CD,GAAe,EACXS,IACHV,GAAU,EACLD,EAAcO,KAClBN,GAAU,EACVD,EAAcO,IAAc,IAG9BR,EAAaT,KAAKa,GAClBH,EAAcV,KAAKqB,MAGdV,EAAS/E,KAAK0F,IAAIV,EAAa,gDA3CbW,EAAUC,GAEnC,IAAMC,EAAY,SAACZ,GAMlB,MADgB,QADhBA,GADAA,GADAA,EAAUE,wBAAcF,IACNa,QAAQ,IAAK,MACbA,QAAQ,IAAK,OACTb,EAAU,MACzBA,GAER,OAAIU,EAASxB,MAAM,WAAYyB,EAASzB,MAAM,UAGvC0B,EAAUF,KAAcE,EAAUD,YCRrCG,EAAQC,SAASC,eAAe,SAChCC,EAASF,SAASC,eAAe,UACjCE,EAAMD,EAAOE,WAAW,MAO1BC,EAAQ,GACRC,EAAuB,EAErBC,EAAyB,SAACC,IAC/BF,GAAwBE,GACG,IAC1BF,EAAuBD,EAAM5D,OAAO,SAACgE,GAAD,OAASA,EAAKC,WAAUpF,OAAS,GAElEgF,EAAuBD,EAAM5D,OAAO,SAACgE,GAAD,OAASA,EAAKC,WAAUpF,OAAS,IACxEgF,EAAuB,IAGnBK,EAA8B,kBACnCN,EAAM5D,OAAO,SAACgE,GAAD,OAASA,EAAKC,WAAUJ,IAEhCM,EAAY,CACjB,IAAIjI,EACJ,IAAIgE,EAAe,CAClB5B,MAAO,EACPM,WC7Ca,mo3BD+Cd,IAAI8B,EAAuB,CAC1BpC,MAAO,IAER,IAAIsC,GAGCwD,EAAkB,WACvBP,EAAuB,EAGvB,IADA,IAAIQ,EAAe,GAHSzE,EAAA,SAInBC,GACR,IAAMyE,EAAWH,EAAUtE,GACrB0E,EAAgBD,EAASE,MAAQ,WACnCC,EAAkBH,EAASI,MAAMpB,EAAMrG,MAAOqG,EAAMqB,gBACnDF,GAEOA,aAA2BrF,MACtCwF,QAAQC,MAAR,GAAAxD,OAAiBkD,EAAjB,cAAAlD,OAA2CoD,EAA3C,0BAEAA,EAAkBA,EAAgBzE,OAAO,SAAC8E,GACzC,MAAsB,kBAAXA,IACVF,QAAQC,MAAR,GAAAxD,OAAiBkD,EAAjB,UAAAlD,OAAuCyD,EAAvC,+CACO,KAMTT,EAAeA,EAAahD,OAAOoD,IAbnCG,QAAQC,MAAR,GAAAxD,OAAiBkD,EAAjB,cAAAlD,OAA2CoD,EAA3C,0BALO5E,EAAI,EAAGA,EAAIsE,EAAUtF,OAAQgB,IAAKD,EAAlCC,GAsBT,IAAIkF,EAAiBnB,EACjBoB,EAAoBX,EACxBT,EAAQ,GAER,IAAK,IAAIqB,EAAI,EAAGA,EAAID,EAAkBnG,OAAQoG,IAE7C,IADA,IAAIC,EAAcF,EAAkBC,GAC3BE,EAAI,EAAGA,EAAIJ,EAAelG,OAAQsG,IAAK,CAC/C,IAAMC,EAAWL,EAAeI,GAChC,GAAIC,EAASC,SAAWH,EAAa,CACpCtB,EAAMjC,KAAKyD,GACXJ,EAAkBM,OAAOL,EAAG,GAAIA,IAChCF,EAAeO,OAAOH,EAAG,GAAIA,IAC7B,OAKH,IAAK,IAAIF,EAAI,EAAGA,EAAID,EAAkBnG,OAAQoG,IAE7C,IADA,IAAMC,EAAcF,EAAkBC,GAC7BE,EAAI,EAAGA,EAAIJ,EAAelG,OAAQsG,IAAK,CAC/C,IAAMC,EAAWL,EAAeI,GAChC,GAAIC,EAASC,OAAO,KAAOH,EAAY,GAAI,CAC1CE,EAASG,WAAWL,GACpBtB,EAAMjC,KAAKyD,GACXJ,EAAkBM,OAAOL,EAAG,GAAIA,IAChCF,EAAeO,OAAOH,EAAG,GAAIA,IAC7B,OAKH,IAAK,IAAIF,EAAI,EAAGA,EAAID,EAAkBnG,OAAQoG,IAAK,CAClD,IACMjB,EAAO,IAAIwB,EADGR,EAAkBC,IAEtCrB,EAAMjC,KAAKqC,GAGZ,IAAMyB,EAAa,IAAI/G,IACvBkF,EAAM3D,QAAQ,SAAC+D,GAAD,OAASyB,EAAWxG,IAAI+E,EAAM0B,EAAS1B,EAAKqB,WAC1DzB,EAAM+B,KAAK,SAACC,EAAQC,GAAT,OAAmBJ,EAAWvG,IAAI2G,GAAUJ,EAAWvG,IAAI0G,MAGjEE,EAAY,IAAIhE,EAAU,CAC/BE,YAAa,KAKR0D,EAAW,SAACR,GAIjB,OAAOY,EAAUJ,SAASR,IAGvBa,EAAgB,EAEdC,EAAY,GACZC,EAAc,GAEhBC,EAAqB,EACrBC,GAAkB,EAElBC,EAAgB,GAChBC,EAAyB,EAEvBC,EAAmB,IAAI5H,IACvB6H,EAAmB,SAACC,GACzB,GAAIF,EAAiBG,IAAID,GACxB,OAAOF,EAAiBpH,IAAIsH,GAE5B,IAAME,EAAenD,SAASoD,cAAc,UACtCC,EAAYF,EAAa/C,WAAW,MAC1CiD,EAAUC,KAAOb,EAAY,WAC7B,IAAMc,EAAQF,EAAUG,YAAYP,GAAMM,MAY1C,OATAJ,EAAaM,YAAcF,EAC3BJ,EAAaI,MAAQA,EAAQ,EAC7BJ,EAAaO,OAAShB,EAAc,EAEpCW,EAAUC,KAAOb,EAAY,WAC7BY,EAAUM,UAAY,OACtBN,EAAUO,aAAe,MACzBP,EAAUQ,SAASZ,EAAM,EAAG,GAC5BF,EAAiBrH,IAAIuH,EAAME,GACpBA,GAIT,SAASlB,EAAKH,GACb7G,KAAK6G,OAASA,EACd7G,KAAK6I,OAAS,GACd,IAAK,IAAIpC,EAAI,EAAGA,EAAII,EAAOxG,OAAQoG,IAAK,CACvC,IAAMuB,EAAOnB,EAAOJ,GACpBzG,KAAK6I,OAAO1F,KAAK,CAChB6E,KAAMA,EACNE,aAAcH,EAAiBC,GAC/Bc,EAAG,EACHC,EAAG,EACHC,IAAK,EACLC,MAAO,EAIPC,MAAO,EACPC,MAAO,EACPC,QAAS,EACTC,SAAU,KAIbrC,EAAKsC,UAAUvC,WAAa,SAASwC,GACpC,IAAMC,EAAaxJ,KAAK6I,OAClBY,EAAazJ,KAAK6G,OAExB7G,KAAK6G,OAAS0C,EACdvJ,KAAK6I,OAAS,GAGd,IADA,IAAIa,EAAmB,EACdjD,EAAI,EAAGA,EAAI8C,EAAWlJ,OAAQoG,IAAK,CAC3C,IAAMkD,EAAWF,EAAWC,GACtB1B,EAAOuB,EAAW9C,GACxB,GAAIuB,IAAS2B,EACZ3J,KAAK6I,OAAO1F,KAAKqG,EAAWE,IAC5BA,QACM,CACN,IAAME,EAAa5J,KAAK6I,OAAO7I,KAAK6I,OAAOxI,OAAS,GACpDL,KAAK6I,OAAO1F,KAAK,CAChB6E,KAAMA,EACNE,aAAcH,EAAiBC,GAC/Bc,EAAGc,EAAaA,EAAWd,EAAIc,EAAW1B,aAAaM,YAAc,EACrEO,EAAGa,EAAaA,EAAWb,EAAI,EAC/BC,IAAK,EACLC,MAAO,EAIPC,MAAO,EACPC,MAAO,EACPC,QAAS,EACTC,SAAU,OAiBdrC,EAAKsC,UAAUO,SAAW,SAASC,EAASC,EAASC,GAEpD,IADA,IAAIC,EAAU,EACLxD,EAAI,EAAGA,EAAIzG,KAAK6I,OAAOxI,OAAQoG,IAAK,CAC5C,IAAMyD,EAAQlK,KAAK6I,OAAOpC,GACpBmD,EAAa5J,KAAK6I,OAAOpC,EAAI,GAC/BmD,IACHK,EAAUL,EAAWO,KAAOP,EAAW1B,aAAaM,aAIrD0B,EAAMC,KAAOF,EAEbC,EAAME,KAAOL,EACbG,EAAMb,SAAWS,GAAW/K,KAAKsL,IAAI,EAAGtL,KAAK0F,IAAI,GAAIuF,EAAoBvD,EAhBxC,IACb,KAoBpByD,EAAMjB,QAAUiB,EAAMb,SAAWa,EAAMjB,OAAS,EAahDiB,EAAMhB,OAVQ,IAUEgB,EAAMC,KAAOD,EAAMpB,GACnCoB,EAAMf,OAXQ,IAWEe,EAAME,KAAOF,EAAMnB,GAmBnCmB,EAAMhB,OAAS,EACfgB,EAAMf,OAAS,EACfe,EAAMpB,GAAKoB,EAAMhB,MACjBgB,EAAMnB,GAAKmB,EAAMf,QAoJnB,IACCrE,EAAMrG,MAAQ6L,aAAa,yBAA2B,GACtD,MAAMC,IACP3E,IAEAd,EAAM0F,iBAAiB,QAAS,WAC/B9C,EAAqB,EACrBC,GAAkB,IAGnB7C,EAAM0F,iBAAiB,QAAS,WAC/B,IACCF,aAAa,wBAA0BxF,EAAMrG,MAC7C,MAAM8L,IACP3E,MAGD6E,OAAOD,iBAAiB,UAAW,SAACD,GACnC,OAAQA,EAAE3K,KACT,IAAK,MACJ,IAAM4F,EAAOE,IACTF,IAvb4BjD,EAwbLiD,EAAKqB,OAvblC/B,EAAM4F,SACN3F,SAAS4F,YAAY,cAAc,EAAOpI,IAwbxC,MACD,IAAK,UACJ+C,GAAwB,GACxB,MACD,IAAK,YACJA,EAAuB,GACvB,MACD,QACC,OAlc+B,IAAC/C,EAoclCgI,EAAEK,mBACA,GAEH3F,EAAOuF,iBAAiB,YAAa,SAACD,GACrCA,EAAEK,iBACF9F,EAAM+F,UACJ,GAjLH,SAASC,EAAQC,GAChBC,sBAAsBF,GARvB,WACC,IAAMxC,EAAQmC,OAAOQ,WACfxC,EAASgC,OAAOS,YAClBjG,EAAOqD,QAAUA,IAAOrD,EAAOqD,MAAQA,GACvCrD,EAAOwD,SAAWA,IAAQxD,EAAOwD,OAASA,GAK9C0C,GAEAjG,EAAIkG,UAAU,EAAG,EAAGnG,EAAOqD,MAAOrD,EAAOwD,QACzCvD,EAAImD,KAAOb,EAAY,MAAQC,EAAc,WAE7C,IAAMlF,EAAOuC,EAAMrG,MAEb4M,EAAYvG,EAAMqB,eAClBmF,EAAUxG,EAAMyG,aAKhBC,EAAYzM,KAAKsL,IAAIgB,EAAWC,GAChCG,EAAY1M,KAAK0F,IAAI4G,EAAWC,GAEhCI,EAASnJ,EAAKjC,MAAM,EAAGkL,GACvBG,EAASpJ,EAAKjC,MAAMkL,EAAWF,GAC/BM,EAAQrJ,EAAKjC,MAAMgL,GAInBO,EAAe3G,EAAIqD,YAAYmD,GAAQpD,MACvCwD,EAAe5G,EAAIqD,YAAYoD,GAAQrD,MAGzC/F,IAASqF,GAAiB0D,IAAYzD,GAA0ByD,IAAYD,IAC/E1D,GAAkB,EAClBD,EAAqB,IAEtBA,GAAsB,GACG,KACxBC,GAAmBA,EACnBD,EAAqB,GAItBH,IAAkBsE,EAAetE,GAAiB,GAElDrC,EAAI6G,OACJ7G,EAAI8G,UAAU/G,EAAOqD,MAAM,EAAGrD,EAAOwD,OAAO,GAC5CvD,EAAI8G,WAAWzE,EAAe,GAC9BrC,EAAI+G,UAAY,0BAChB/G,EAAIgH,SAASL,GAAerE,EAAWsE,EAAcrE,GACrDvC,EAAI+G,UAAY,QAChB/G,EAAI0D,SAAS8C,EAAQ,EAAG,GACxBxG,EAAI+G,UAAY,QAChB/G,EAAI0D,SAAS+C,EAAQE,EAAc,GACnC3G,EAAI+G,UAAY,QAChB/G,EAAI0D,SAASgD,EAAOC,EAAeC,EAAc,GAEjD,IAAMK,EAAgBpH,SAASqH,gBAAkBtH,GAASC,SAASsH,WAC/D1E,GAAmBwE,GAClBb,IAAYD,GACfnG,EAAIgH,SAASL,GAAerE,EAAW,EAAGC,GAU5C,IADA,IAAIsC,EAAU,EACL1I,EAAI,EAAGA,EAAI+D,EAAM/E,OAAQgB,IAAK,CACtC,IAAMmE,EAAOJ,EAAM/D,GAcbyI,EAAoE,IAA1DtE,EAAKqB,OAAOpE,cAAc6J,QAAQ/J,EAAKE,gBAAwB+C,EAAKqB,SAAWtE,EAC/FiD,EAAKC,SAAWqE,EAChBtE,EAAK+G,oBAAsB7G,MAAkCF,EAC7DA,EAAKqE,SAASC,EAASC,EAAS0B,GAGhC,IAAK,IAAIhF,EAAI,EAAGA,EAAIjB,EAAKqD,OAAOxI,OAAQoG,IAAK,CAI5C,IAAMyD,EAAQ1E,EAAKqD,OAAOpC,GACpByB,EAAegC,EAAMhC,aAC3BhD,EAAIsH,YAActC,EAAMjB,MACpBzD,EAAK+G,sBACRrH,EAAI6G,OACJ7G,EAAI+G,UAAY,yBAChB/G,EAAIgH,SAAShC,EAAMpB,EAAGoB,EAAMnB,EAAGb,EAAaM,YAAaf,GACzDvC,EAAIuH,WASLvH,EAAIwH,UAAUxE,EAAcgC,EAAMpB,EAAGoB,EAAMnB,GAIxCe,IACHC,GAAWtC,GAGbvC,EAAIsH,YAAc,EAElBtH,EAAIuH,UAGJ7E,EAAgBrF,EAChBsF,EAAyByD,EA0D1BR","file":"static/js/main.351fbcfe.chunk.js","sourcesContent":["\r\nconst psuedo_db = [\r\n\t\"Hello world!\",\r\n\t\"That quick brown fox was jumping all around\",\r\n\t\"The quick brown fox was jumping all around\",\r\n\t\"The quick brown fox jumps over lazydawgs\",\r\n\t\"The quick brown fox jumps over a lazy dog\",\r\n\t\"Sphinx of black quartz, judge my vow.\",\r\n\t\"The previous pigeon tempers the crystal answer.\",\r\n\t\"Over the definite minimalist overlaps this grateful drama.\",\r\n\t\"A shortened analogue baffles the percentage on top of the acoustic client.\",\r\n\t\"An abbreviated analog confuses the rate on top of the acoustic customer.\",\r\n\t\"Will the client deduce the modern paint?\",\r\n\t\"An egg attacks!\",\r\n\t\"Does the outstanding immortal reach past the absolute?\",\r\n\t\"Our still-competitor lands the aircraft next to his opening taste.\",\r\n\t\"A sushi-centric motif, but with so many pairs of chopsticks, and only one piece of sushi\",\r\n\t\"What is this trying to say? There are ways of thinking that don't exist yet\",\r\n\t\"What is this trying to say? There are ways of thinking that haven't been invented\",\r\n\t\"What is this trying to say? There are ways of thinking that haven't been discovered\",\r\n\t\"What is this trying to say? There are ways of thinking that haven't been thought of\",\r\n\t\"What is this trying to say? There are ways of thinking that haven't been thought of yet\",\r\n\t\"What are you trying to say?\",\r\n\t\"Who are *you*?\",\r\n\t\"This isn't trying to solve a problem.\",\r\n\t\"I want to explore.\",\r\n\t\"I implore you to explore the text galore\",\r\n\t\"where there's text, there's always more\",\r\n\t\"there's always more, there's always more\",\r\n\t\"there's always more... there's always more...\",\r\n\t\"The medium is the message.\",\r\n\t\"The Medium is the Message\",\r\n\t\"The Medium is the Massage\",\r\n\t\"The Medium is the Mess Age\",\r\n\t\"The Medium is the Mass Age\",\r\n\t\"Diverge at every letter\",\r\n\t\"Diverge at every word and any letter\",\r\n\t\"Diverge at any word or letter\",\r\n\t\"alphabeta-magneta\",\r\n\t\"alphabetic-magnetic\",\r\n\t\"alphabetically magnetic\",\r\n\t\"start from nothing\",\r\n\t\"start from nothing, end up with something\",\r\n\t\"start from nothing, end up somewhere you didn't expect\",\r\n\t\"start from something, end up somewhere you didn't expect\",\r\n\t\"Textploration incarnate.\",\r\n\t\"A new medium of text.\",\r\n\t\"A new medium for text.\",\r\n\t\"A medium for massaging text.\",\r\n\t\"A new medium for textploration.\",\r\n\t\"It's a bit of a surprise to see you again for the first time.\",\r\n\t\"Once upon a time,\",\r\n\t\"Once upon a time, there was a\",\r\n\t\"Once upon a time, there was a time that wasn't once; it was twice and it was thrice, all because of a device.\",\r\n\t\"Once upon a time, there was a time that wasn't once; it was twice and it was thrice, all because of this device. It was\",\r\n\t\"The utter minimalism of a centered blinking cursor is appealing.\",\r\n\t\"while the world was not perfect, we could and would make it better.\",\r\n\t\"i can work with that.\",\r\n\t\"QWERTYUIOPASDFGHJKLZXCVBNM 1234567890 `~-_=+[{]}\\\\|;:'\\\",<.>/?\",\r\n\t\"qwertopia\",\r\n\t\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\r\n\t\"asdlkjlakjdflskjdf\",\r\n\t\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec fringilla sed arcu id consequat. Nulla nec urna sed lectus semper cursus. Nunc quis rhoncus elit, sed tincidunt odio. Sed ornare fringilla sem, non fringilla nibh sagittis eget. Integer commodo nulla elit, in efficitur augue venenatis eget. Proin id metus vel neque convallis sagittis eget eu turpis. Nulla lacinia nec sem et posuere. Aliquam ex magna, lacinia sed erat a, accumsan ullamcorper nunc. Integer non est sodales, mollis lorem egestas, molestie mauris. Fusce pharetra leo sit amet urna porttitor, at eleifend tortor maximus.\",\r\n\t\"]0301/134429.526:ERROR:exception_handler_server.cc(524)] ConnectNamedPipe: The pipe is being closed. (0xE8)\"\r\n];\r\n\r\nexport default class DatabaseProvider {\r\n\tquery() {\r\n\t\treturn psuedo_db;\r\n\t}\r\n}\r\n","const sumWeights = (weights)=> {\r\n\tlet sum = 0;\r\n\tfor (const value of weights.values()) {\r\n\t\tsum += value;\r\n\t}\r\n\treturn sum;\r\n};\r\n\r\nconst weightedSample = (weights, sumTotalWeight)=> {\r\n\tconst n = Math.random() * sumTotalWeight;\r\n\tlet threshold = 0;\r\n\tfor (const [key, weightValue] of weights.entries()) {\r\n\t\tthreshold += weightValue;\r\n\t\tif (n < threshold) {\r\n\t\t\treturn key;\r\n\t\t}\r\n\t}\r\n};\r\n\r\nexport class Markov {\r\n\tconstructor(order) {\r\n\t\tthis.order = order; // the n in \"ngrams\"; 2 = digraphs, 3 = trigraphs\r\n\t\tthis.ngrams = new Map(); // ngrams to counts\r\n\t}\r\n\r\n\ttrain(corpusText) {\r\n\t\tif (corpusText.length > 0) {\r\n\t\t\tcorpusText += corpusText.slice(0, this.order); // wrap around\r\n\t\t\twhile (corpusText.length < this.order) { // handle too small training inputs\r\n\t\t\t\tcorpusText += corpusText.slice(0, this.order);\r\n\t\t\t}\r\n\t\t}\r\n\t\tconst length = corpusText.length;\r\n\t\tfor (let index = 0; index < length - this.order; index++) {\r\n\t\t\tconst ngram = corpusText.slice(index, index + this.order);\r\n\t\t\tthis.ngrams.set(ngram, (this.ngrams.get(ngram) || 0) + 1);\r\n\t\t}\r\n\t\tthis.ngram_keys = Array.from(this.ngrams.keys());\r\n\t\tthis.sum_total_weight = sumWeights(this.ngrams);\r\n\t}\r\n\r\n\tcontinueText(current_text, length_to_add) {\r\n\t\tconst target_length = current_text.length + length_to_add;\r\n\t\tif (current_text.length < this.order) {\r\n\t\t\t// TOmaybeDO: could find ngrams that start with part of the end of current_text\r\n\t\t\t// also maybe don't go over the target length\r\n\t\t\tcurrent_text += weightedSample(this.ngrams, this.sum_total_weight);\r\n\t\t}\r\n\t\tfor (let i = 0; i < length_to_add && current_text.length < target_length; i++) {\r\n\t\t\tconst lastChars = current_text.slice(current_text.length - this.order + 1);\r\n\t\t\tconst nextCharsToWeights = new Map();\r\n\t\t\tthis.ngram_keys\r\n\t\t\t\t.filter((ngram)=> {\r\n\t\t\t\t\treturn lastChars === ngram.slice(0, this.order - 1);\r\n\t\t\t\t\t// attempt at optimization:\r\n\t\t\t\t\t// for (let ci = 0, end = this.order - 1; ci < end; ci++) {\r\n\t\t\t\t\t// \tif (lastChars.charCodeAt(ci) !== ngram.charCodeAt(ci)) {\r\n\t\t\t\t\t// \t\treturn false\r\n\t\t\t\t\t// \t}\r\n\t\t\t\t\t// }\r\n\t\t\t\t\t// return true;\r\n\t\t\t\t})\r\n\t\t\t\t.forEach((ngram)=> {\r\n\t\t\t\t\tnextCharsToWeights.set(ngram.slice(this.order - 1), this.ngrams.get(ngram));\r\n\t\t\t\t});\r\n\t\t\tcurrent_text += (\r\n\t\t\t\tweightedSample(nextCharsToWeights, sumWeights(nextCharsToWeights)) ||\r\n\t\t\t\tweightedSample(this.ngrams, this.sum_total_weight)\r\n\t\t\t);\r\n\t\t}\r\n\t\treturn current_text;\r\n\t}\r\n}\r\n\r\nexport default class MarkovProvider {\r\n\tconstructor({order, corpusText}) {\r\n\t\tthis.markov = new Markov(order);\r\n\t\tthis.markov.train(corpusText);\r\n\t}\r\n\tquery(current_text, index) {\r\n\t\t// TODO: maybe continue from cursor (index)?\r\n\t\t// current_text = current_text.slice(0, index);\r\n\t\treturn new Array(5).fill(0).map(()=> this.markov.continueText(current_text, 20));\r\n\t}\r\n}\r\n\r\nexport class MarkovSelfTextProvider {\r\n\tconstructor({order}) {\r\n\t\tthis.order = order;\r\n\t}\r\n\tquery(current_text, index) {\r\n\t\tconst markov = new Markov(this.order);\r\n\t\tmarkov.train(current_text);\r\n\t\t// TODO: maybe continue from cursor (index)?\r\n\t\t// current_text = current_text.slice(0, index);\r\n\t\treturn new Array(3).fill(0).map(()=> markov.continueText(current_text, 20));\r\n\t}\r\n}\r\n","export default class OriginalJokeProvider {\r\n\tconstructor() {\r\n\t\t// this.suffix = \"...in bed\";\r\n\t\t// this.suffix = \"...Laaast Niiiiiight\";\r\n\t\tthis.suffix = \"(no pun intended)\"; // https://xkcd.com/559/\r\n\t}\r\n\tquery(current_text, index) {\r\n\t\treturn [current_text + \" \" + this.suffix];\r\n\t}\r\n}\r\n","import wordsToPhonemes from \"cmu-pronouncing-dictionary\";\r\n\r\nexport function guessPhonemesForWord(word) {\r\n\tword = word.toLowerCase();\r\n\t// research actual letter-to-sound (LtoS) rules\r\n\t// ideally find a module for this\r\n\t// actually, a speech synth's rules would be good\r\n\tconst phonemes = [];\r\n\tlet inVowels = false;\r\n\tfor (let i = 0; i < word.length; i++) {\r\n\t\tlet isVowel = false;\r\n\t\tif (word[i].match(/[aeiou]/)) {\r\n\t\t\tisVowel = true;\r\n\t\t} else if(word[i] === \"y\") {\r\n\t\t\tif (inVowels) {\r\n\t\t\t\t// \"hey\"\r\n\t\t\t\tisVowel = true;\r\n\t\t\t} else if (word[i + 1] && word[i + 1].match(/[aeiou]/)) {\r\n\t\t\t\t//\t\"yes\"\r\n\t\t\t\tisVowel = false;\r\n\t\t\t} else {\r\n\t\t\t\t// \"yttrium\"\r\n\t\t\t\tisVowel = true;\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tisVowel = false;\r\n\t\t}\r\n\t\tif (isVowel && (!inVowels || i === 0)) {\r\n\t\t\tphonemes.push(\"@\");\r\n\t\t}\r\n\t\tif (!isVowel && (inVowels || i === 0)) {\r\n\t\t\tphonemes.push(\"$\");\r\n\t\t}\r\n\t\tinVowels = isVowel;\r\n\t}\r\n\treturn phonemes;\r\n}\r\n\r\nexport function findOrGuessPhonemesForWord(word) {\r\n\tword = word.toLowerCase();\r\n\t// TODO: maybe handle alternate phoneme expansions/possibilities like a(1) = EY1\r\n\t// (and give the benefit of the doubt as to things being matches)\r\n\tconst phonemesString = wordsToPhonemes[word];\r\n\tif (phonemesString) {\r\n\t\treturn phonemesString.split(\" \");\r\n\t}\r\n\t// console.log(`not found in pronunciation dictionary: '${word}' (going to guess)`);\r\n\treturn guessPhonemesForWord(word);\r\n}\r\n\r\nexport function toPhonemes(text) {\r\n\tconst words = text.toLowerCase().split(/[\\s,./;:\"\\\\[\\]!.\\-_+|?]+/) // TODO: better tokenizer/splitter\r\n\t\t.filter((word)=> word);\r\n\t// return words.flatMap(getPhonemesForWord);\r\n\tlet phonemes = [];\r\n\twords.forEach((word)=> {\r\n\t\tphonemes = phonemes.concat(findOrGuessPhonemesForWord(word));\r\n\t});\r\n\treturn phonemes;\r\n}\r\n","import {isVowelish, stripStressor} from \"phoneme-types\";\r\nimport {toPhonemes} from \"../phonics\"\r\n\r\n// TODO: generalize to alliteration\r\nexport default class Assonance {\r\n\tconstructor({maxDistance, minDistanceBeforeCloserIsBetter, closerIsBetterFactor}) {\r\n\t\t// TODO: decide off-by-one-ish-ness-es for distance params, whatever makes sense\r\n\t\tthis.windowSize = maxDistance;\r\n\t\tthis.minDistanceBeforeCloserIsBetter = minDistanceBeforeCloserIsBetter;\r\n\t\tthis.closerIsBetterFactor = closerIsBetterFactor;\r\n\t}\r\n\t// or vowelsAreAssonant\r\n\tstatic testVowelAssonance(phoneme1, phoneme2) {\r\n\t\t// TODO: actually critically decide which sounds should be considered similar\r\n\t\tconst normalize = (phoneme)=> {\r\n\t\t\t// make the similar the same\r\n\t\t\tphoneme = stripStressor(phoneme);\r\n\t\t\tphoneme = phoneme.replace(/R/, \"H\");\r\n\t\t\tphoneme = phoneme.replace(/W/, \"\");\r\n\t\t\tif (phoneme === \"AH\") phoneme = \"EH\"; // should this be here?\r\n\t\t\treturn phoneme;\r\n\t\t}\r\n\t\tif (phoneme1.match(/[@$?]/) || phoneme2.match(/[@$?]/)) {\r\n\t\t\treturn false;\r\n\t\t}\r\n\t\treturn normalize(phoneme1) === normalize(phoneme2);\r\n\t}\r\n\tevaluate(text) {\r\n\t\tconst phonemes = toPhonemes(text);\r\n\t\tconst vowelHistory = [];\r\n\t\tconst pointsHistory = [];\r\n\t\tlet points = 0;\r\n\t\tlet totalVowels = 0;\r\n\t\tphonemes.forEach((phoneme)=> {\r\n\t\t\tif (isVowelish(stripStressor(phoneme)) || phoneme === \"@\") {\r\n\t\t\t\tif (vowelHistory.length > this.windowSize) {\r\n\t\t\t\t\tvowelHistory.shift();\r\n\t\t\t\t\tpointsHistory.shift();\r\n\t\t\t\t}\r\n\t\t\t\tconst matchIndex = vowelHistory.findIndex((recentVowel)=> {\r\n\t\t\t\t\treturn Assonance.testVowelAssonance(recentVowel, phoneme);\r\n\t\t\t\t});\r\n\t\t\t\tconst matchedRecentVowel = matchIndex > -1 && phoneme !== \"@\";\r\n\t\t\t\ttotalVowels += 1;\r\n\t\t\t\tif (matchedRecentVowel) {\r\n\t\t\t\t\tpoints += 1;\r\n\t\t\t\t\tif (!pointsHistory[matchIndex]) {\r\n\t\t\t\t\t\tpoints += 1;\r\n\t\t\t\t\t\tpointsHistory[matchIndex] = true;\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tvowelHistory.push(phoneme);\r\n\t\t\t\tpointsHistory.push(matchedRecentVowel);\r\n\t\t\t}\r\n\t\t});\r\n\t\treturn points / Math.max(totalVowels, 1);\r\n\t}\r\n}\r\n","import DatabaseProvider from \"./providers/DatabaseProvider\";\r\nimport MarkovProvider, {MarkovSelfTextProvider} from \"./providers/MarkovProvider\";\r\nimport OriginalJokeProvider from \"./providers/OriginalJokeProvider\";\r\nimport Assonance from \"./evaluators/Assonance\";\r\n// import Lipogram from \"./evaluators/Lipogram\";\r\n/* eslint import/no-webpack-loader-syntax: off */\r\nimport corpusText from \"!!raw-loader!./corpus.txt\";\r\n\r\n// FIXME: the Menu key opens a menu with the context of the canvas instead of the input in chrome\r\n// chrome apparently triggers a secondary click at the focused element's location\r\n// so it has to be on top and have pointer-events and everything\r\n// TODO: we want the menu to open up in a reasonable location\r\n// so we'll need to position the input\r\n// and if we're positioning the input, maybe we can just use that\r\n// and get pointer-based selection and other benefits\r\n\r\n// const container = document.getElementById(\"root\");\r\nconst input = document.getElementById(\"input\");\r\nconst canvas = document.getElementById(\"canvas\");\r\nconst ctx = canvas.getContext(\"2d\");\r\n\r\nconst setTextKeepingUndoHistory = (text)=> {\r\n\tinput.select();\r\n\tdocument.execCommand(\"insertText\", false, text);\r\n};\r\n\r\nlet paths = [];\r\nlet autocompletion_index = 0;\r\n\r\nconst cycleAutocompletePaths = (direction)=> {\r\n\tautocompletion_index += direction;\r\n\tif (autocompletion_index < 0) {\r\n\t\tautocompletion_index = paths.filter((path)=> path._visible).length - 1;\r\n\t}\r\n\tif (autocompletion_index > paths.filter((path)=> path._visible).length - 1) {\r\n\t\tautocompletion_index = 0;\r\n\t}\r\n}\r\nconst getSelectedAutocompletePath = ()=>\r\n\tpaths.filter((path)=> path._visible)[autocompletion_index];\r\n\r\nconst providers = [\r\n\tnew DatabaseProvider(),\r\n\tnew MarkovProvider({\r\n\t\torder: 5,\r\n\t\tcorpusText,\r\n\t}),\r\n\tnew MarkovSelfTextProvider({\r\n\t\torder: 3,\r\n\t}),\r\n\tnew OriginalJokeProvider(),\r\n];\r\n\r\nconst query_providers = ()=> {\r\n\tautocompletion_index = 0;\r\n\r\n\tlet path_strings = [];\r\n\tfor (let i = 0; i < providers.length; i++) {\r\n\t\tconst provider = providers[i];\r\n\t\tconst provider_name = provider.name || \"Provider\";\r\n\t\tlet providerResults = provider.query(input.value, input.selectionStart);\r\n\t\tif (!providerResults) {\r\n\t\t\tconsole.error(`${provider_name} returned ${providerResults} instead of an array`);\r\n\t\t} else if (!providerResults instanceof Array) {\r\n\t\t\tconsole.error(`${provider_name} returned ${providerResults} instead of an array`);\r\n\t\t} else {\r\n\t\t\tproviderResults = providerResults.filter((result)=> {\r\n\t\t\t\tif (typeof result !== \"string\") {\r\n\t\t\t\t\tconsole.error(`${provider_name} gave ${result} instead of a string for one of the paths`);\r\n\t\t\t\t\treturn false;\r\n\t\t\t\t}\r\n\t\t\t\t// TODO: keep metadata for debug / understanding\r\n\t\t\t\t// result._provider_ = provider;\r\n\t\t\t\treturn true;\r\n\t\t\t});\r\n\t\t\tpath_strings = path_strings.concat(providerResults);\r\n\t\t}\r\n\t}\r\n\r\n\tlet old_paths_left = paths;\r\n\tlet path_strings_left = path_strings;\r\n\tpaths = [];\r\n\r\n\tfor (let j = 0; j < path_strings_left.length; j++) {\r\n\t\tlet path_string = path_strings_left[j];\r\n\t\tfor (let k = 0; k < old_paths_left.length; k++) {\r\n\t\t\tconst old_path = old_paths_left[k];\r\n\t\t\tif (old_path.string === path_string) {\r\n\t\t\t\tpaths.push(old_path);\r\n\t\t\t\tpath_strings_left.splice(j, 1); j--;\r\n\t\t\t\told_paths_left.splice(k, 1); k--;\r\n\t\t\t\tbreak;\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tfor (let j = 0; j < path_strings_left.length; j++) {\r\n\t\tconst path_string = path_strings_left[j];\r\n\t\tfor (let k = 0; k < old_paths_left.length; k++) {\r\n\t\t\tconst old_path = old_paths_left[k];\r\n\t\t\tif (old_path.string[0] === path_string[0]) {\r\n\t\t\t\told_path.set_string(path_string);\r\n\t\t\t\tpaths.push(old_path);\r\n\t\t\t\tpath_strings_left.splice(j, 1); j--;\r\n\t\t\t\told_paths_left.splice(k, 1); k--;\r\n\t\t\t\tbreak;\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tfor (let j = 0; j < path_strings_left.length; j++) {\r\n\t\tconst path_string = path_strings_left[j];\r\n\t\tconst path = new Path(path_string);\r\n\t\tpaths.push(path);\r\n\t}\r\n\r\n\tconst judgements = new Map();\r\n\tpaths.forEach((path)=> judgements.set(path, evaluate(path.string)));\r\n\tpaths.sort((path_a, path_b)=> judgements.get(path_b) - judgements.get(path_a));\r\n};\r\n\r\nconst assonance = new Assonance({\r\n\tmaxDistance: 10,\r\n});\r\n// const lipogram = new Lipogram(\"e\");\r\n// const topWordsOnly = new TopWordsOnly();\r\n\r\nconst evaluate = (path_string)=> {\r\n\t// if (!lipogram.evaluate(path_string)) {\r\n\t// \treturn 0;\r\n\t// }\r\n\treturn assonance.evaluate(path_string);\r\n};\r\n\r\nlet view_center_x = 0;\r\n\r\nconst font_size = 20;\r\nconst line_height = 25;\r\n\r\nlet cursor_blink_timer = 0;\r\nlet cursor_blink_on = true;\r\n\r\nlet previous_text = \"\";\r\nlet previous_selection_end = 0;\r\n\r\nconst glyph_canvas_map = new Map();\r\nconst get_glyph_canvas = (char)=> {\r\n\tif (glyph_canvas_map.has(char)) {\r\n\t\treturn glyph_canvas_map.get(char);\r\n\t} else {\r\n\t\tconst glyph_canvas = document.createElement(\"canvas\");\r\n\t\tconst glyph_ctx = glyph_canvas.getContext(\"2d\");\r\n\t\tglyph_ctx.font = font_size + \"px Arial\";\r\n\t\tconst width = glyph_ctx.measureText(char).width;\r\n\t\t// TODO: use width based the surrounding characters for kerning (ideally)\r\n\t\t// the +5 to width below is mainly for f\r\n\t\tglyph_canvas.glyph_width = width;\r\n\t\tglyph_canvas.width = width + 5;\r\n\t\tglyph_canvas.height = line_height + 5;\r\n\r\n\t\tglyph_ctx.font = font_size + \"px Arial\";\r\n\t\tglyph_ctx.textAlign = \"left\";\r\n\t\tglyph_ctx.textBaseline = \"top\";\r\n\t\tglyph_ctx.fillText(char, 0, 5);\r\n\t\tglyph_canvas_map.set(char, glyph_canvas);\r\n\t\treturn glyph_canvas;\r\n\t}\r\n};\r\n\r\nfunction Path(string) {\r\n\tthis.string = string;\r\n\tthis.glyphs = [];\r\n\tfor (let j = 0; j < string.length; j++) {\r\n\t\tconst char = string[j];\r\n\t\tthis.glyphs.push({\r\n\t\t\tchar: char,\r\n\t\t\tglyph_canvas: get_glyph_canvas(char),\r\n\t\t\tx: 0,\r\n\t\t\ty: 0,\r\n\t\t\trot: 0,\r\n\t\t\talpha: 0,\r\n\t\t\t// x_to: 0,\r\n\t\t\t// y_to: 0,\r\n\t\t\t// rot_to: 0,\r\n\t\t\tx_vel: 0,\r\n\t\t\ty_vel: 0,\r\n\t\t\trot_vel: 0,\r\n\t\t\talpha_to: 0,\r\n\t\t});\r\n\t}\r\n}\r\nPath.prototype.set_string = function(new_string) {\r\n\tconst old_glyphs = this.glyphs;\r\n\tconst old_string = this.string;\r\n\r\n\tthis.string = new_string;\r\n\tthis.glyphs = [];\r\n\r\n\tlet old_string_index = 0;\r\n\tfor (let j = 0; j < new_string.length; j++) {\r\n\t\tconst old_char = old_string[old_string_index];\r\n\t\tconst char = new_string[j];\r\n\t\tif (char === old_char) {\r\n\t\t\tthis.glyphs.push(old_glyphs[old_string_index]);\r\n\t\t\told_string_index++;\r\n\t\t} else {\r\n\t\t\tconst prev_glyph = this.glyphs[this.glyphs.length - 1];\r\n\t\t\tthis.glyphs.push({\r\n\t\t\t\tchar: char,\r\n\t\t\t\tglyph_canvas: get_glyph_canvas(char),\r\n\t\t\t\tx: prev_glyph ? prev_glyph.x + prev_glyph.glyph_canvas.glyph_width : 0,\r\n\t\t\t\ty: prev_glyph ? prev_glyph.y : 0,\r\n\t\t\t\trot: 0,\r\n\t\t\t\talpha: 0,\r\n\t\t\t\t// x_to: 0,\r\n\t\t\t\t// y_to: 0,\r\n\t\t\t\t// rot_to: 0,\r\n\t\t\t\tx_vel: 0,\r\n\t\t\t\ty_vel: 0,\r\n\t\t\t\trot_vel: 0,\r\n\t\t\t\talpha_to: 0,\r\n\t\t\t});\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// TODO: use Hirschberg's algorithm for sequence alignment\r\n// https://en.wikipedia.org/wiki/Hirschberg%27s_algorithm\r\n\r\n// Path.prototype.matchTo = function(string) {\r\n// \tconst dist = Levenshtein.get(this.string, string);\r\n// \treturn dist / (Math.max(this.string.length, string.length) + 1);\r\n// }\r\n\r\nconst completely_faded_out_point = 30;\r\nconst fade_out_over = 20;\r\n\r\nPath.prototype.simulate = function(matched, place_y, selection_end_pos) {\r\n\tlet place_x = 0;\r\n\tfor (let j = 0; j < this.glyphs.length; j++) {\r\n\t\tconst glyph = this.glyphs[j];\r\n\t\tconst prev_glyph = this.glyphs[j - 1];\r\n\t\tif (prev_glyph) {\r\n\t\t\tplace_x = prev_glyph.x_to + prev_glyph.glyph_canvas.glyph_width;\r\n\t\t\t// this will give a squishy rollout, and works better with a faster transition\r\n\t\t\t// place_x = prev_glyph.x + prev_glyph.glyph_canvas.glyph_width;\r\n\t\t}\r\n\t\tglyph.x_to = place_x;\r\n\t\t// glyph.x_to = matched && place_x;\r\n\t\tglyph.y_to = place_y;// + Math.sin(place_x / 50) * 5;\r\n\t\tglyph.alpha_to = matched && Math.min(1, Math.max(0, (selection_end_pos - j + completely_faded_out_point) / fade_out_over));\r\n\t\t// glyph.x += (glyph.x_to - glyph.x) / 20;\r\n\t\t// // glyph.x += (glyph.x_to - glyph.x) / 5;\r\n\t\t// glyph.y += (glyph.y_to - glyph.y) / 15;\r\n\t\t// glyph.alpha += (glyph.alpha_to - glyph.alpha) / 10;\r\n\t\tglyph.alpha += (glyph.alpha_to - glyph.alpha) / 4;\r\n\t\t// place_x += glyph_canvas.glyph_width;\r\n\r\n\t\tconst force = 1/2;\r\n\t\tconst damping = 2;\r\n\t\t// const force = 1/50;\r\n\t\t// const damping = 0.1;\r\n\t\t// const force = 1/20;\r\n\t\t// const damping = 0.2;\r\n\t\t// const force = 1/20;\r\n\t\t// const damping = 1;\r\n\t\t// const force = 1/200;\r\n\t\t// const damping = 1;\r\n\t\tglyph.x_vel += (glyph.x_to - glyph.x) * force;\r\n\t\tglyph.y_vel += (glyph.y_to - glyph.y) * force;\r\n\t\t// const funForce = 6;\r\n\t\t// glyph.x_vel += Math.sin(j/8 + Date.now() / 500) * funForce;\r\n\t\t// glyph.y_vel += Math.cos(j/8 + Date.now() / 500) * funForce;\r\n\t\t// const magneticForce = 1/2;\r\n\t\t// paths.forEach((other_path)=> {\r\n\t\t// \tif (other_path !== this) {\r\n\t\t// \t\tother_path.glyphs.forEach((other_glyph)=> {\r\n\t\t// \t\t\tif (other_glyph.char === glyph.char) {\r\n\t\t// \t\t\t\tif (Math.abs(other_glyph.x - glyph.x) + Math.abs(other_glyph.y - glyph.y) < 10) {\r\n\t\t// \t\t\t\t\t// const distance = Math.max(1, Math.hypot(other_glyph.y - glyph.y, other_glyph.x - glyph.x));\r\n\t\t// \t\t\t\t\tconst distance = Math.max(1, Math.hypot(other_glyph.y - glyph.y, other_glyph.x - glyph.x) ** 2);\r\n\t\t// \t\t\t\t\tglyph.x_vel += (other_glyph.x - glyph.x) / distance * magneticForce;\r\n\t\t// \t\t\t\t\tglyph.y_vel += (other_glyph.y - glyph.y) / distance * magneticForce;\r\n\t\t// \t\t\t\t}\r\n\t\t// \t\t\t}\r\n\t\t// \t\t});\r\n\t\t// \t}\r\n\t\t// });\r\n\t\tglyph.x_vel /= 1 + damping;\r\n\t\tglyph.y_vel /= 1 + damping;\r\n\t\tglyph.x += glyph.x_vel;\r\n\t\tglyph.y += glyph.y_vel;\r\n\t}\r\n}\r\n\r\nfunction resize() {\r\n\tconst width = window.innerWidth;\r\n\tconst height = window.innerHeight;\r\n\tif (canvas.width !== width) canvas.width = width;\r\n\tif (canvas.height !== height) canvas.height = height;\r\n}\r\n\r\nfunction animate(t) {\r\n\trequestAnimationFrame(animate);\r\n\tresize();\r\n\r\n\tctx.clearRect(0, 0, canvas.width, canvas.height);\r\n\tctx.font = font_size + \"px/\" + line_height + \"px Arial\";\r\n\r\n\tconst text = input.value;\r\n\r\n\tconst start_pos = input.selectionStart;\r\n\tconst end_pos = input.selectionEnd;\r\n\t// NOTE: selectionStart amd selectionEnd are already min and max indexes\r\n\t// (NOT \"where you started\" and \"where you're selecting to\")\r\n\t// there's selectionDirection which can be \"forward\", \"backward\", or \"none\"\r\n\r\n\tconst lower_pos = Math.min(start_pos, end_pos);\r\n\tconst upper_pos = Math.max(start_pos, end_pos);\r\n\r\n\tconst before = text.slice(0, lower_pos);\r\n\tconst inside = text.slice(lower_pos, end_pos);\r\n\tconst after = text.slice(end_pos);\r\n\r\n\t// const all_text_metrics = ctx.measureText(text);\r\n\t// console.log(all_text_metrics.fontBoundingBoxDescent);\r\n\tconst before_width = ctx.measureText(before).width;\r\n\tconst inside_width = ctx.measureText(inside).width;\r\n\t// const after_width = ctx.measureText(after).width;\r\n\r\n\tif (text !== previous_text || end_pos !== previous_selection_end || end_pos !== start_pos) {\r\n\t\tcursor_blink_on = true;\r\n\t\tcursor_blink_timer = 0; // could be negative\r\n\t}\r\n\tcursor_blink_timer += 1;\r\n\tif (cursor_blink_timer > 40) {\r\n\t\tcursor_blink_on = !cursor_blink_on;\r\n\t\tcursor_blink_timer = 0;\r\n\t}\r\n\r\n\t// TODO: center the controlled end of the selection (need to check input.selectionDirection)\r\n\tview_center_x += (before_width - view_center_x) / 20;\r\n\r\n\tctx.save();\r\n\tctx.translate(canvas.width/2, canvas.height/2);\r\n\tctx.translate(-view_center_x, 0);\r\n\tctx.fillStyle = \"rgba(0, 120, 255, 0.56)\";\r\n\tctx.fillRect(before_width, -font_size, inside_width, line_height);\r\n\tctx.fillStyle = \"black\";\r\n\tctx.fillText(before, 0, 0);\r\n\tctx.fillStyle = \"white\";\r\n\tctx.fillText(inside, before_width, 0);\r\n\tctx.fillStyle = \"black\";\r\n\tctx.fillText(after, before_width + inside_width, 0);\r\n\r\n\tconst input_focused = document.activeElement === input && document.hasFocus();\r\n\tif (cursor_blink_on && input_focused) {\r\n\t\tif (end_pos === start_pos) {\r\n\t\t\tctx.fillRect(before_width, -font_size, 2, line_height);\r\n\t\t}\r\n\t\t// if (input.selectionDirection === \"backward\") {\r\n\t\t// \tctx.fillRect(before_width, -font_size, 2, line_height);\r\n\t\t// } else if (input.selectionDirection === \"forward\") {\r\n\t\t// \tctx.fillRect(before_width + inside_width, -font_size, 2, line_height);\r\n\t\t// }\r\n\t}\r\n\r\n\tlet place_y = 0;\r\n\tfor (let i = 0; i < paths.length; i++) {\r\n\t\tconst path = paths[i];\r\n\t\t// let place_y = (1 + i) * line_height;\r\n\t\t// console.log(path.matchTo(text))\r\n\t\t// if (path.matchTo(text) > 0.9) {\r\n\t\t// \tplace_y = innerHeight;\r\n\t\t// }\r\n\r\n\t\t// let str_dist = Levenshtein.get(path.string, text);\r\n\r\n\t\t// TODO: uniquify truncated strings,\r\n\t\t// and probably weigh paths higher if there are multiple results for it\r\n\t\t// and/or visually indicate that case specifically somehow\r\n\t\t// TODO: actually match paths together and show them branching off\r\n\t\t// if (path.string.toLowerCase().indexOf(text.toLowerCase()) === 0) {\r\n\t\tconst matched = path.string.toLowerCase().indexOf(text.toLowerCase()) === 0 && path.string !== text;\r\n\t\tpath._visible = matched;\r\n\t\tpath.autoCompleteHilight = getSelectedAutocompletePath() === path;\r\n\t\tpath.simulate(matched, place_y, upper_pos);\r\n\t\t// ctx.rotate(0.04);\r\n\t\t// ctx.rotate(0.04 * (path.glyphs[0] && path.glyphs[0].alpha));\r\n\t\tfor (let j = 0; j < path.glyphs.length; j++) {\r\n\t\t\t// ctx.rotate(0.001);\r\n\t\t\t// ctx.save();\r\n\t\t\t// ctx.rotate(-0.1);\r\n\t\t\tconst glyph = path.glyphs[j];\r\n\t\t\tconst glyph_canvas = glyph.glyph_canvas;\r\n\t\t\tctx.globalAlpha = glyph.alpha;\r\n\t\t\tif (path.autoCompleteHilight) {\r\n\t\t\t\tctx.save();\r\n\t\t\t\tctx.fillStyle = \"rgba(255, 255, 0, 0.5)\";\r\n\t\t\t\tctx.fillRect(glyph.x, glyph.y, glyph_canvas.glyph_width, line_height);\r\n\t\t\t\tctx.restore();\r\n\t\t\t}\r\n\t\t\t// ctx.rotate(0.002 * glyph.alpha);\r\n\t\t\t// ctx.rotate(-0.002 * glyph.alpha * (1 + 0.1 * (i%10)));\r\n\t\t\t// ctx.rotate(0.002 * glyph.alpha * (1 + 0.1 * (i%10)));\r\n\t\t\t// ctx.rotate(0.002 * glyph.alpha * (1 + 10 * (i%10)));\r\n\t\t\t// ctx.rotate(0.002 * glyph.alpha * (1 + 10 * (j%10)));\r\n\t\t\t// ctx.rotate(0.002 * glyph.alpha * (1 + 100 * (j%10)));\r\n\t\t\t// FIXME: blurry text\r\n\t\t\tctx.drawImage(glyph_canvas, glyph.x, glyph.y);\r\n\t\t\t// ctx.drawImage(glyph_canvas, ~~glyph.x, ~~glyph.y);\r\n\t\t\t// ctx.restore();\r\n\t\t}\r\n\t\tif (matched) {\r\n\t\t\tplace_y += line_height;\r\n\t\t}\r\n\t}\r\n\tctx.globalAlpha = 1;\r\n\r\n\tctx.restore();\r\n\r\n\r\n\tprevious_text = text;\r\n\tprevious_selection_end = end_pos;\r\n\r\n}\r\n\r\n/*function fullscreen() {\r\n\tif (container.requestFullscreen) {\r\n\t\tcontainer.requestFullscreen();\r\n\t} else if (container.msRequestFullscreen) {\r\n\t\tcontainer.msRequestFullscreen();\r\n\t} else if (container.mozRequestFullScreen) {\r\n\t\tcontainer.mozRequestFullScreen();\r\n\t} else if (container.webkitRequestFullscreen) {\r\n\t\tcontainer.webkitRequestFullscreen();\r\n\t}\r\n}*/\r\n\r\ntry{\r\n\tinput.value = localStorage[\"diverge current path\"] || \"\";\r\n}catch(e){}\r\nquery_providers();\r\n\r\ninput.addEventListener(\"focus\", ()=> {\r\n\tcursor_blink_timer = 0;\r\n\tcursor_blink_on = true;\r\n});\r\n\r\ninput.addEventListener(\"input\", ()=> {\r\n\ttry{\r\n\t\tlocalStorage[\"diverge current path\"] = input.value;\r\n\t}catch(e){}\r\n\tquery_providers();\r\n});\r\n\r\nwindow.addEventListener(\"keydown\", (e)=> {\r\n\tswitch (e.key) {\r\n\t\tcase \"Tab\":\r\n\t\t\tconst path = getSelectedAutocompletePath();\r\n\t\t\tif (path) {\r\n\t\t\t\tsetTextKeepingUndoHistory(path.string);\r\n\t\t\t}\r\n\t\t\tbreak;\r\n\t\tcase \"ArrowUp\":\r\n\t\t\tcycleAutocompletePaths(-1);\r\n\t\t\tbreak;\r\n\t\tcase \"ArrowDown\":\r\n\t\t\tcycleAutocompletePaths(+1);\r\n\t\t\tbreak;\r\n\t\tdefault:\r\n\t\t\treturn; // don't prevent default\r\n\t}\r\n\te.preventDefault();\r\n}, false);\r\n\r\ncanvas.addEventListener(\"mousedown\", (e)=> {\r\n\te.preventDefault();\r\n\tinput.focus();\r\n}, false);\r\n\r\nanimate();\r\n","export default \"If I needed someone to love\\r\\nYou're the one that I'd be thinking of\\r\\nIf I needed someone\\r\\n\\r\\nIf I had some more time to spend\\r\\nThen I guess I'd be with you my friend\\r\\nIf I needed someone\\r\\nHad you come some other day\\r\\nThen it might not have been like this\\r\\nBut you see now I'm too much in love\\r\\n\\r\\nCarve your number on my wall\\r\\nAnd maybe you will get a call from me\\r\\nIf I needed someone\\r\\nAh, ah, ah, ah\\r\\n\\r\\nIf I had some more time to spend\\r\\nThen I guess I'd be with you my friend\\r\\nIf I needed someone\\r\\nHad you come some other day\\r\\nThen it might not have been like this\\r\\nBut you see now I'm too much in love\\r\\n\\r\\nCarve your number on my wall\\r\\nAnd maybe you will get a call from me\\r\\nIf I needed someone\\r\\n\\r\\nKBpedia is now open source!\\r\\nKBpedia Knowledge Structure\\r\\n\\r\\n\\r\\nHOME\\r\\nKNOWLEDGE GRAPH\\r\\nDEMO\\r\\nBACKGROUND\\r\\nUSE CASES\\r\\nRESOURCES\\r\\n\\r\\n  Use Cases Word Embedding Corpuses\\r\\n\\r\\nUSE CASE\\r\\nTitle:\\tDocument-specific word2Vec Training Corpuses\\r\\nShort Description:\\tThe rich structure in KBpedia is used to create training corpuses for word2vec rapidly and cheaply on the fly\\r\\nProblem:\\tWe need to cluster or classify documents by topic, or to characterize them by sentiment or for recommendations\\r\\nApproach:\\tword2vec is an artificial intelligence 'word embedding' model that can establish similarities between terms. These similarities can be used to address the stated problems. The rich structure and entity types within KBpedia's knowledge structure can be used, with one or two simple queries, to create relevant domain \\\"slices\\\" of tens of thousands of documents and entities upon which to train word2vec models. This approach eliminates the majority of effort normally associated with word2vec for domain purposes, enabling available effort to be spent on refining the parameters of the model for superior results\\r\\nKey Findings:\\r\\nDomain-specifc training corpuses work better with less ambiguity than general corpuses for these problems\\r\\nKBpedia speeds and eases the creation of domain-specific training corpuses for word2vec (and other corpus-based models)\\r\\nOther public and private text sources may be readily added to the KBpedia baseline in order to obtain still further domain-relevant models\\r\\nSuch domain-specific training corpuses can be used to establish similarity between local text documents or HTML web pages\\r\\nThis method can also be combined with a topics analyzer to first tag text documents using KBpedia reference concepts, and then inform or augment these domain-specific training corpuses\\r\\nThese capabilities enable rapid testing and refinement of different combinations of \\\"seed\\\" concepts to obtain better desired results.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nAccording to DeepLearning4J's Word2Vec tutorial, \\\"Given enough data, usage and contexts, Word2vec can make highly accurate guesses about a word’s meaning based on past appearances. Those guesses can be used to establish a word’s association with other words (e.g., 'man' is to 'boy' what 'woman' is to 'girl'), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management.\\\"\\r\\n\\r\\nWord2vec is a two layer artificial neural network used to process text to learn relationships between words within a text corpus. Word2vec takes as its input a large corpus of text and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space. This \\\"word embedding\\\" approach is able to capture multiple different degrees of similarity between words. To create the model of relationships between the words, a particular grouping of text or documents is fed to the word2vec process, which is called the training corpus.\\r\\n\\r\\nThis use case shows how the KBpedia knowledge structure can be used to automatically create highly accurate domain-specific training corpuses that can be used by word2vec to generate word relationship models, often with superior performance and results to generalized word2vec models. The basic approach in this use case is not only applicable to word2vec, but to any method that uses corpuses of text for training. For example, in another use case, we will show how this can be done with another algorithm called ESA (Explicit Semantic Analysis).\\r\\n\\r\\nIt is said about word2vec that \\\"given enough data, usage and contexts, word2vec can make highly accurate guesses about a word’s meaning based on past appearances.\\\" What this use case shows is how the context of the training corpus may greatly impact the results. This use case also shows how KBpedia may be leveraged to quickly create very responsive domain-specific corpuses for training the model.\\r\\n\\r\\n\\r\\nTraining Corpus\\r\\nA training corpus is really just a set of text used to train unsupervised machine learning algorithms. Any kind of text can be used by word2vec. The only thing it does is to learn the relationships between the words that exist in the text. However, not all training corpuses are equal. Training corpuses are often dirty, biaised and ambiguous. Depending on the task at hand, it may be exactly what is required, but more often than not, such errors need to be fixed. Cognonto has the advantage of starting with clean text.\\r\\n\\r\\nWhen we want to create a new training corpus, the first step is to find a source of text that could work to create that corpus. The second step is to select the text we want to add to it. The third step is to pre-process that corpus of text to perform different operations on the text, such as: removing HTML elements; removing punctuation; normalizing text; detecting named entities; etc. The final step is to train word2vec to generate the model.\\r\\n\\r\\nWord2vec is somewhat dumb. It only learns what exists in the training corpus. It does not do anything other than \\\"read\\\" the text and then analyze the relationships between the words (which are really just groups of characters separated by spaces). The word2vec process is highly subject to the Garbage In, Garbage Out principle, which means that if the training set is dirty, biaised and ambiguous, then the learned relationship will end-up being of little or no value.\\r\\n\\r\\n\\r\\nDomain-specific Training Corpus\\r\\nA domain-specific training corpus is a specialized training corpus where its text is related to a specific domain. Examples of domains are music, mathematics, cars, healthcare, etc. In contrast, a general training corpus is a corpus of text that may contain text that discusses totally different domains. By creating a corpus of text that covers a specific domain of interest, we limit the usage of words (that is, their co-occurrences) to texts that are meaningful to that domain.\\r\\n\\r\\nAs we will see in this use case, a domain-specific training corpus can be quite useful, and much more powerful, than general ones, if the task at hand is in relation to a specific domain of expertise. In the past, the major problem with domain-specific training corpuses was that they were costly to create. These costs arose because it is necessary to find a source of data to use, and then to select the specific documents to include in the training corpus. This can work if we want a corpus with 100 or 200 documents, but what if you want a training corpus of 100,000 or 200,000 documents? Then it becomes a problem.\\r\\n\\r\\nThis is the kind of problem that KBpedia helps to resolve. KBpedia is a set of ~39,000 reference concepts that have ~138,000 links to schema of external data sources such as Wikipedia, Wikidata and USPTO. It is that structure and these links to external data sources that we use to create domain-specific training corpuses on the fly. We leverage the reference concept structure to select all of the concepts that should be part of the domain that is being defined. Then we use inference capabilities to infer all of the thousands of concepts that define the full scope of the domain. Then we analyze the hundreds or thousands of concepts we selected that way to get all of the links to external data sources. Finally we use these references to create the training corpus. All of this is done automatically once the initial few concepts that define the subject domain get selected. The workflow looks like:\\r\\n\\r\\ncognonto-workflow.png\\r\\n\\r\\nThe Process\\r\\nTo show how this process works, let's create a domain-specific training corpus using KBpedia about, say, musicians. We will compare this domain-specific corpus to the general word2vec model created by Google based on news sources that has about 100 billion words. The Google model contains 300-dimensional vectors for 3 million words and phrases. We will use the Google News model as the general model to compare the results/performance to our domain-specific musicians model.\\r\\n\\r\\n\\r\\nDetermining the Domain\\r\\nThe first step is to define the scope of the domain we want to create. For this use case example, we want a domain that is somewhat constrained to create a training corpus that is not too large for demo purposes. The domain we have chosen is musicians. This domain is related to people and bands that play music. It is also related to musical genres, instruments, music industry, etc.\\r\\n\\r\\nTo create this domain, we beginwith a single KBpedia reference concept: Musician. If we want to broaden the scope of the domain, we could have included other concepts such as: Music, Musical Group, Musical Instrument, etc.\\r\\n\\r\\n\\r\\nAggregating the Domain-specific Training Corpus\\r\\nOnce we have determined the scope of the domain, the next step is to query the KBpedia knowledge base to aggregate all of the text that will belong to that training corpus. The end result of this operation is to create a training corpus with text that is only related to the scope of the domain we defined.\\r\\n\\r\\n(defn create-domain-specific-training-set\\r\\n  [target-kbpedia-class corpus-file]\\r\\n  (let [step 1000\\r\\n        entities-dataset \\\"http://kbpedia.org/knowledge-base/\\\"\\r\\n        kbpedia-dataset \\\"http://kbpedia.org/kko/\\\"\\r\\n        nb-entities (get-nb-entities-for-class-ws target-kbpedia-class entities-dataset kbpedia-dataset)]\\r\\n    (loop [nb 0\\r\\n           nb-processed 1]\\r\\n      (when (< nb nb-entities)\\r\\n        (doseq [entity (get-entities-slice target-kbpedia-class entities-dataset kbpedia-dataset :limit step :offset @nb-processed)]\\r\\n          (spit corpus-file (str (get-entity-content entity) \\\"\\\\n\\\") :append true)\\r\\n          (println (str nb-processed \\\"/\\\" nb-entities)))\\r\\n        (recur (+ nb step)\\r\\n               (inc nb-processed))))))\\r\\n\\r\\n(create-domain-specific-training-set \\\"http://kbpedia.org/kko/rc/Musician\\\" \\\"resources/musicians-corpus.txt\\\")\\r\\nWhat this code does is to query the KBpedia knowledge base to get all the named entities that are linked to it, for the scope of the domain we defined. Then the text related to each entity is appended to a text file where each line is the text of a single entity.\\r\\n\\r\\nGiven the scope of the current use case, the musicians training corpus is composed of 47,263 documents. With a simple function, we are able to aggregate 47,263 text documents highly related to a conceptual domain we defined on the fly. All of the hard work has been delegated to the knowledge base and its conceptual structure. (In fact, this simple function leverages 8 years of hard work).\\r\\n\\r\\n\\r\\nNormalizing Text\\r\\nThe next step is a common one related to any NLP pipeline. Before learning from the training corpus, we should clean and normalize the text of its raw form.\\r\\n\\r\\n(defn normalize-proper-name\\r\\n  [name]\\r\\n  (-> name\\r\\n      (string/replace #\\\" \\\" \\\"_\\\")\\r\\n      (string/lower-case)))\\r\\n\\r\\n(defn pre-process-line\\r\\n  [line]\\r\\n  (-> (let [line (-> line\\r\\n                     ;; 1. remove all underscores\\r\\n                     (string/replace \\\"_\\\" \\\" \\\"))]\\r\\n        ;; 2. detect named entities and change them with their underscore form, like: Fred Giasson -> fred_giasson\\r\\n        (loop [entities (into [] (re-seq #\\\"[\\\\p{Lu}]([\\\\p{Ll}]+|\\\\.)(?:\\\\s+[\\\\p{Lu}]([\\\\p{Ll}]+|\\\\.))*(?:\\\\s+[\\\\p{Ll}][\\\\p{Ll}\\\\-]{1,3}){0,1}\\\\s+[\\\\p{Lu}]([\\\\p{Ll}]+|\\\\.)\\\" line))\\r\\n               line line]\\r\\n          (if (empty? entities)\\r\\n            line\\r\\n            (let [entity (first (first entities))]\\r\\n              (recur (rest entities)\\r\\n                     (string/replace line entity (normalize-proper-name entity)))))))\\r\\n      (string/replace (re-pattern stop-list) \\\" \\\")\\r\\n      ;; 4. remove everything between brackets like: [1] [edit] [show]\\r\\n      (string/replace #\\\"\\\\[.*\\\\]\\\" \\\" \\\")\\r\\n      ;; 5. punctuation characters except the dot and the single quote, replace by nothing: (),[]-={}/\\\\~!?%$@&*+:;<>\\r\\n      (string/replace #\\\"[\\\\^\\\\(\\\\)\\\\,\\\\[\\\\]\\\\=\\\\{\\\\}\\\\/\\\\\\\\\\\\~\\\\!\\\\?\\\\%\\\\$\\\\@\\\\&\\\\*\\\\+:\\\\;\\\\<\\\\>\\\\\\\"\\\\p{Pd}]\\\" \\\" \\\")\\r\\n      ;; 6. remove all numbers\\r\\n      (string/replace #\\\"[0-9]\\\" \\\" \\\")\\r\\n      ;; 7. remove all words with 2 characters or less\\r\\n      (string/replace #\\\"\\\\b[\\\\p{L}]{0,2}\\\\b\\\" \\\" \\\")\\r\\n      ;; 10. normalize spaces\\r\\n      (string/replace #\\\"\\\\s{2,}\\\" \\\" \\\")\\r\\n      ;; 11. normalize dots with spaces\\r\\n      (string/replace #\\\"\\\\s\\\\.\\\" \\\".\\\")\\r\\n      ;; 12. normalize dots\\r\\n      (string/replace #\\\"\\\\.{1,}\\\" \\\".\\\")\\r\\n      ;; 13. normalize underscores\\r\\n      (string/replace #\\\"\\\\_{1,}\\\" \\\"_\\\")\\r\\n      ;; 14. remove standalone single quotes\\r\\n      (string/replace \\\" ' \\\" \\\" \\\")\\r\\n      ;; 15. re-normalize spaces\\r\\n      (string/replace #\\\"\\\\s{2,}\\\" \\\" \\\")\\r\\n      ;; 16. put everything lowercase\\r\\n      (string/lower-case)\\r\\n\\r\\n      (str \\\"\\\\n\\\")))\\r\\n\\r\\n(defn pre-process-corpus\\r\\n  [in-file out-file]\\r\\n  (spit out-file \\\"\\\" :append true)\\r\\n  (with-open [file (clojure.java.io/reader in-file)]\\r\\n    (doseq [line (line-seq file)]\\r\\n      (spit out-file (pre-process-line line) :append true))))\\r\\n\\r\\n(pre-process-corpus \\\"resources/musicians-corpus.txt\\\" \\\"resources/musicians-corpus.clean.txt\\\")\\r\\nWe remove all of the characters that may cause issues to the tokenizer used by the word2vec implementation. We also remove unnecessary words and other words that appear too often or that add nothing to the model we want to generate (like the listing of days and months). We also drop all numbers. By the way, such cleaning steps are common to most such models, and can be used repeatedly across projects.\\r\\n\\r\\n\\r\\nTraining word2vec\\r\\nThe last step is to train word2vec on our clean domain-specific training corpus to generate the model we will use. For this use case, we will use the DL4J (Deep Learning for Java) library that is a Java implementation of the word2vec methods. Training word2vec is as simple as using the DL4J API like this:\\r\\n\\r\\n(defn train\\r\\n  [training-set-file model-file]\\r\\n  (let [sentence-iterator (new LineSentenceIterator (clojure.java.io/file training-set-file))\\r\\n        tokenizer (new DefaultTokenizerFactory)\\r\\n        vec (.. (new word2vec$Builder)\\r\\n                (minWordFrequency 1)\\r\\n                (windowSize 5)\\r\\n                (layerSize 100)\\r\\n                (iterate sentence-iterator)\\r\\n                (tokenizerFactory tokenizer)\\r\\n                build)]\\r\\n    (.fit vec)\\r\\n    (SerializationUtils/saveObject vec (io/file model-file))\\r\\n    vec))\\r\\n\\r\\n(def musicians-model (train \\\"resources/musicians-corpus.clean.txt\\\" \\\"resources/musicians-corpus.model\\\"))\\r\\nWhat is important to notice here is the number of parameters that can be defined to train word2vec on a corpus. In fact, word2vec can be sensitive to parametrization. In a standard use case, since creation of the domain-specific training corpus is so easy, most of the total time getting great results is spent tuning these parameters (subjects of other use cases).\\r\\n\\r\\n\\r\\nImporting the General Model\\r\\nTo provide our general comparison, we next need to import the Google News model. DL4J can import this model without having to generate it ourselves (in fact, only the model is distributed by Google, not the training corpus):\\r\\n\\r\\n(defn import-google-news-model\\r\\n  []\\r\\n  (org.deeplearning4j.models.embeddings.loader.WordVectorSerializer/loadGoogleModel (clojure.java.io/file \\\"GoogleNews-vectors-negative300.bin.gz\\\") true))\\r\\n\\r\\n(def google-news-model (import-google-news-model))\\r\\n\\r\\nPlaying With Models\\r\\nNow that we have a domain-specific model related to musicians and a general model related to news processed by Google, let's start playing with both to see how they perform on different tasks. In the following examples, we will always compare the domain-specific training corpus with the general one.\\r\\n\\r\\n\\r\\nAmbiguous Words\\r\\nA characteristic of words is that their surface form can be ambiguous; they can have multiple meanings. An ambiguous word can co-occur with multiple other words that may not have any shared meaning. But all of this depends on the context. If we are in a general context, then this situation will happen more often than we think and will impact the similarity score of these ambiguous terms. However, as we will see, this phenomenum is greatly diminished when we use domain-specific models.\\r\\n\\r\\n\\r\\nSimilarity Between Piano, Organ and Violin\\r\\nWhat we want to check is the relationship between 3 different music instruments: piano, organ and violin. We want to check the relationship between each of them.\\r\\n\\r\\n(.similarity musicians-model \\\"piano\\\" \\\"violin\\\")\\r\\n0.8810334205627441\\r\\n(.similarity musicians-model \\\"piano\\\" \\\"organ\\\")\\r\\n0.8591226935386658\\r\\nAs we can see, both tuples have a high likelihood of co-occurrence. This suggests that these terms of each tuple are probably highly related. In this case, it is probably because violins are often played along with a piano. And, it is probable that an organ looks like a piano (at least it has a keyboard).\\r\\n\\r\\nNow let's take a look at what the general model has to say about that:\\r\\n\\r\\n(.similarity google-news-model \\\"piano\\\" \\\"violin\\\")\\r\\n0.8228187561035156\\r\\n(.similarity google-news-model \\\"piano\\\" \\\"organ\\\")\\r\\n0.4616874158382416\\r\\nThe surprising fact here is the apparent dissimilarity between piano and organ compared with the results we got with the musicians domain-specific model. If we think a bit about this use case, we will probably conclude that these results makes sense. In fact, organ is an ambiguous word in a general context. An organ can be a musical instrument, but it can also be a part of an anatomy. This means that the word organ will co-occur with piano, but also to all other kinds of words related to human and animal biology. This is why they are less similar in the general model than in the domain one, because it is an ambiguous word in a general context.\\r\\n\\r\\n\\r\\nSimilarity Between Album and Track\\r\\nNow let's see another similarity example between two other words album and track where track is an ambiguous word depending on the context.\\r\\n\\r\\n(.similarity musicians-model \\\"album\\\" \\\"track\\\")\\r\\n0.838570237159729\\r\\n(.similarity google-news-model \\\"album\\\" \\\"track\\\")\\r\\n0.18461625277996063\\r\\nAs expected, because track is ambiguous, there is a big difference in terms of co-occurence probabilities depending on the context (domain-specific or general).\\r\\n\\r\\n\\r\\nSimilarity Between Pianist and Violinist\\r\\nHowever, are domain-specific and general differences always the case? Let's take a look at two words that are domain specific and unambiguous: pianist and violinist.\\r\\n\\r\\n(.similarity musicians-model \\\"pianist\\\" \\\"violinist\\\")\\r\\n0.8497374653816223\\r\\n(.similarity google-news-model \\\"pianist\\\" \\\"violinist\\\")\\r\\n0.8616064190864563\\r\\nIn this case, the similarity score between the two terms is almost the same. In both contexts (generals and domain specific), their co-occurrence is similar.\\r\\n\\r\\n\\r\\nNearest Words\\r\\nNow let's look at the similarity between two distinct words in two new and distinct contexts. Let's take a look at a few words and see what other words occur most often with them.\\r\\n\\r\\n\\r\\nMusic\\r\\n(.wordsNearest musicians-model [\\\"music\\\"] [] 20)\\r\\nmusic\\r\\nmusical\\r\\nvocal\\r\\norchestral\\r\\nvoice\\r\\ndance.\\r\\nartistic\\r\\nwidely\\r\\nromantic\\r\\nmodern\\r\\ncabaret\\r\\ntheatrical\\r\\nbelongs\\r\\nopera\\r\\ngenres.\\r\\nfilm\\r\\nmiddle_eastern\\r\\nsongs.\\r\\nspecialised\\r\\ngenre\\r\\n\\r\\n\\r\\n(.wordsNearest google-news-model [\\\"music\\\"] [] 20)\\r\\nmusic\\r\\nclassical_music\\r\\njazz\\r\\nMusic\\r\\nWithout_Donny_Kirshner\\r\\nsongs\\r\\nmusicians\\r\\ntunes\\r\\nmusical\\r\\nLogue_typed\\r\\nmusics\\r\\nunplugged_acoustic\\r\\nfunky_soulful\\r\\nincludes_didgeridoo_riff\\r\\nsoulful_melodies\\r\\nhip_hop\\r\\n_nova_samba\\r\\nbossa\\r\\nreggae\\r\\njazz_ragtime\\r\\n\\r\\n\\r\\nOne observation we can make is that the terms from the musicians model are more general than the ones from the general model.\\r\\n\\r\\n\\r\\nTrack\\r\\n(.wordsNearest musicians-model [\\\"track\\\"] [] 20)\\r\\ntrack\\r\\nalbum\\r\\nreleased.\\r\\ntitled\\r\\ndebut\\r\\nspawned\\r\\nentitled\\r\\ntrack.\\r\\nlatest\\r\\nhit.\\r\\nrelease\\r\\nweek\\r\\nyear.\\r\\ndania\\r\\nsong\\r\\nsummer_of_space_on_quiet\\r\\npositive\\r\\nairplay\\r\\ncity_productions.\\r\\ntracks\\r\\n\\r\\n\\r\\n(.wordsNearest google-news-model [\\\"track\\\"] [] 20)\\r\\ntrack\\r\\ntracks\\r\\nTrack\\r\\nracetrack\\r\\nwww.southbostonspeedway.com\\r\\ncuppy\\r\\n#/#ths-mile\\r\\nhorseshoe_shaped_section\\r\\nwagering_parlors\\r\\nlevigated\\r\\nInfineon_raceway\\r\\nTezgam_Express_heading\\r\\n#/##-mile_oval\\r\\nracing\\r\\npresident_Brandon_Igdalsky\\r\\npaperclip_shaped\\r\\ncinder_track\\r\\npresident_Gillian_Zucker\\r\\n2_#/#-mile_triangular\\r\\nTapeta_surface\\r\\n\\r\\n\\r\\nAs we know, track is ambiguous. The difference between these two sets of nearest related words is striking. There is a clear conceptual correlation in the musicians' domain-specific model. But in the general model, it is really going in all directions.\\r\\n\\r\\n\\r\\nYear\\r\\nNow let's take a look at a really general word: year\\r\\n\\r\\n(.wordsNearest musicians-model [\\\"year\\\"] [] 20)\\r\\nyear\\r\\ngrammy_award_for_best\\r\\nnaacap\\r\\nyear.\\r\\ngrammy_award\\r\\nnominated\\r\\nrock_new_artist_clip\\r\\nmusic_award\\r\\ngrammy_for_best\\r\\ncategory.\\r\\nsong_of_the\\r\\nmusic_video_awards_best_pop\\r\\nghantous.\\r\\nsalvador_da_bahia.\\r\\nwon\\r\\nso_intense\\r\\nentitled\\r\\nhe_was_grammy\\r\\nyear_and_recorded\\r\\nsong\\r\\n\\r\\n\\r\\n(.wordsNearest google-news-model [\\\"year\\\"] [] 20)\\r\\nyear\\r\\nmonth\\r\\nweek\\r\\nmonths\\r\\ndecade\\r\\nyears\\r\\nsummer\\r\\nyear.The\\r\\nSeptember\\r\\nweeks\\r\\nseason\\r\\nJune\\r\\nyaer\\r\\nweekend\\r\\nJuly\\r\\nJanuary\\r\\ntwoyears\\r\\nAugust\\r\\nthreeyear\\r\\nOctober\\r\\n\\r\\n\\r\\nThis one is quite interesting too. Both groups of words makes sense, but only in their respective contexts. With the musicians' model, year is mostly related to awards (like the Grammy Awards 2016), categories like \\\"song of the year\\\", etc.\\r\\n\\r\\nIn the context of the general model, year is really related to time concepts: months, seasons, etc.\\r\\n\\r\\n\\r\\nPlaying With Co-Occurrences Vectors\\r\\nFinally we will play with manipulating the co-occurrences vectors by manipulating them. A really popular word2vec equation is king - man + women = queen. What is happening under the hood with this equation is that we are adding and substracting the co-occurence \\\"vectors\\\" for each of these words, and we check the nearest word of the resulting co-occurence vector.\\r\\n\\r\\nNow, let's take a look at a few of these equations.\\r\\n\\r\\n\\r\\nPianist + Renowned = ?\\r\\n(.wordsNearest musicians-model [\\\"pianist\\\" \\\"renowned\\\"] [] 10)\\r\\nrenowned\\r\\npianist\\r\\nteacher\\r\\nteacher.\\r\\nprolific\\r\\neducator.\\r\\nvirtuoso\\r\\nviolinist\\r\\nconductor\\r\\ncomposer\\r\\n\\r\\n\\r\\n(.wordsNearest google-news-model [\\\"pianist\\\" \\\"renowned\\\"] [] 10)\\r\\nrenowned\\r\\npianist\\r\\npianist_composer\\r\\njazz_pianist\\r\\nclassical_pianists\\r\\ncomposer_pianist\\r\\nvirtuoso_pianist\\r\\nrenowned_cellist\\r\\nviolinist\\r\\ntrumpet_virtuoso\\r\\n\\r\\n\\r\\nThese kinds of operations are also interesting. If we add the two co-occurrence vectors for pianist and renowned then we get that a teacher, an educator, a composer or a virtuoso is a renowned pianist.\\r\\n\\r\\nFor unambiguous surface forms like pianist, then the two models score quite well. The difference between the two examples comes from the way the general training corpus has been created (pre-processed) compared to the musicians corpus.\\r\\n\\r\\n\\r\\nMetal + Death = ?\\r\\n(.wordsNearest musicians-model [\\\"metal\\\" \\\"death\\\"] [] 10)\\r\\ndeath\\r\\nmetal\\r\\nthrash\\r\\ndeathcore\\r\\nmetalcore\\r\\ngrindcore\\r\\nmathcore\\r\\nmelodic\\r\\npresent_labels_insideout\\r\\ngothic\\r\\n\\r\\n\\r\\n(.wordsNearest google-news-model [\\\"metal\\\" \\\"death\\\"] [] 10)\\r\\ndeath\\r\\nmetal\\r\\nTunstall_bled\\r\\nsteel\\r\\nDeath\\r\\ncompressional_asphyxia\\r\\nuntimely_death\\r\\nthallium_toxic\\r\\nmetal_grindcore\\r\\nblackened_shards\\r\\n\\r\\n\\r\\nThis example uses two quite general words with no apparent relationship between them. The results with the musicians' model are all the highly similar genre of music like trash metal, deathcore metal, etc.\\r\\n\\r\\nHowever with the general model, it is a mix of multiple unrelated concepts.\\r\\n\\r\\n\\r\\nMetal - Death + Smooth = ?\\r\\nLet's play some more with these equations. What if we want some kind of smooth metal?\\r\\n\\r\\n(.wordsNearest musicians-model [\\\"metal\\\" \\\"smooth\\\"] [\\\"death\\\"] 5)\\r\\nsmooth\\r\\nfunk\\r\\nsoul\\r\\ndisco\\r\\ndaniel_håkansson_genres\\r\\n\\r\\n\\r\\nThis one is also quite interesting. We substracted the death co-occurrence vector to the metal one, and then we added the smooth vector. What we end-up with is a bunch of music genres that are much smoother than death metal.\\r\\n\\r\\n(.wordsNearest google-news-model [\\\"metal\\\" \\\"smooth\\\"] [\\\"death\\\"] 5)\\r\\nsmooth\\r\\nmetal\\r\\nshredding_solos\\r\\nchromed_steel\\r\\nmetallic\\r\\nIn the case of the general model, we end-up with \\\"smooth metal\\\". The removal of the death vector has no effect on the results, probably since these are three ambiguous and really general terms.\\r\\n\\r\\n\\r\\nRelevance of the Use Case\\r\\nOf course, the likelihood that musicians are a domain of interest to most enterprises is low. However, this use case does have these implications:\\r\\n\\r\\nThe speed and ease of creating domain-specific training corpuses for word2vec (and other corpus-based models)\\r\\nThe ability to include other public and private text sources into the word2vec model (see, for example, how to link your own private datasets to KBpedia)\\r\\nTo use such domain-specific training corpuses to establish similarity between local text documents or HTML web pages\\r\\nTo combine with a topics analyzer to first tag text document using KBpedia reference concepts, and then inform or augment domain-specific training corpuses, and\\r\\nTo enable the testing and refinement of different combinations of \\\"seed\\\" concepts to produce training corpuses with the desired set of similarity results.\\r\\n\\r\\nConclusion\\r\\nAs we saw, creating domain-specific training corpuses to use with word2vec can have a dramatic impact on the results and how results can be much more meaningful within the scope of that domain. Another advantage of a domain-specific training corpus is it creates create much smaller models. Smaller models are faster to generate, faster to download/upload, faster to query, and consume less memory.\\r\\n\\r\\nOf the concepts in KBpedia, roughly 47,000 of them correspond to types (or classes) of various sorts. These pre-determined slices are available across all needs and domains to generate such domain-specific corpuses. Further, KBpedia is designed for rapid incorporation of your own domain information to add further to this discriminatory power.\\r\\n\\r\\n\\r\\n\\r\\nKBpedia\\r\\n\\r\\nKBpedia exploits large-scale knowledge bases and semantic technologies for machine learning, data interoperability and mapping, and fact extraction and tagging.\\r\\n\\r\\nLatest News\\r\\nOpen-source Baseline for KBpedia Now Available\\r\\n02/04/2019\\r\\nKBpedia is Now Open Source\\r\\n10/23/2018\\r\\nKBpedia v. 151 Released\\r\\n09/13/2017\\r\\nOther Resources\\r\\nABOUT\\r\\nFAQ\\r\\nNEWS\\r\\nUSE CASES\\r\\nDOCUMENTATION\\r\\nPRIVACY\\r\\nTERMS OF USE\\r\\nContact Us\\r\\nc/o Cognonto Corp.\\r\\n380 Knowling Drive\\r\\nCoralville, IA 52241\\r\\nU.S.A.\\r\\nVoice: +1 319 621 5225\\r\\n2016-2019 © Cognonto Corp.Cognonto Corp. All Rights Reserved.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\""],"sourceRoot":""}